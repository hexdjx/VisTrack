{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad3c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "# print(torch.__version__)\n",
    "# print(torch.version.cuda)\n",
    "# print(torch.backends.cudnn.version())\n",
    "# print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d056d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可复现性\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c378bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISBLE_DEVICES'] = '0,1' # 命令行指定gpu device: CUDA_VISBLE_DEVICES=0,1 python *.py\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tensor = torch.randn(3,5,5)\n",
    "# print(tensor)\n",
    "# print(tensor.to(device))\n",
    "# print(tensor.cuda()) # .cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52900f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c57e2477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# 张量处理\n",
    "tensor = torch.randn(3,4,5)\n",
    "# print(tensor)\n",
    "print(tensor.type())\n",
    "print(tensor.long().type()) # float()\n",
    "# print(tensor.size())\n",
    "# print(tensor.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dba3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python image libirary PIL<-->numpy<-->torch Image:[0,1] float <--> [0,255] int\n",
    "\n",
    "import PIL \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "tensor = torch.randn(3,28,28)\n",
    "\n",
    "tensor = tensor.cuda()\n",
    "ndarray = tensor.cpu().numpy()\n",
    "# print(ndarray)\n",
    "tensor = torch.from_numpy(ndarray)\n",
    "\n",
    "# [H W 3]-->[3 H W]\n",
    "image = PIL.Image.fromarray(torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy())\n",
    "# image1 = torchvision.transforms.functional.to_pil_image(tensor) #与上面等价\n",
    "\n",
    "# image.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# cv2.imshow('im', np.asarray(image))\n",
    "# key = cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0999611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.from_numpy(np.asarray(PIL.Image.open('fig.jpg'))).permute(2,0,1).float()/255\n",
    "# print(tensor)\n",
    "tensor1 = torchvision.transforms.functional.to_tensor(PIL.Image.open('fig.jpg'))\n",
    "# print(tensor1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5827cfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7682])\n",
      "0.08847743272781372\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(0)\n",
    "print(torch.rand(1))\n",
    "# torch.manual_seed(0)\n",
    "print(torch.rand(1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3be5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512])\n",
      "64\n",
      "torch.Size([64, 512])\n",
      "64\n",
      "torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "# view/reshape 和 shape/size对比\n",
    "tensor = torch.rand(2,3,4)\n",
    "tensor = torch.reshape(tensor, (6,4)) \n",
    "tensor1 = tensor.view(6,4)\n",
    "tensor = torch.rand(64,512)\n",
    "print(tensor.shape)\n",
    "print(tensor.shape[0])\n",
    "\n",
    "print(tensor.size())\n",
    "print(tensor.size(0))\n",
    "\n",
    "print(tensor1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b23c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 7.],\n",
      "        [3., 4., 8.],\n",
      "        [5., 6., 9.]])\n",
      "tensor([0, 2, 1])\n",
      "tensor([[1., 2., 7.],\n",
      "        [3., 4., 8.],\n",
      "        [5., 6., 9.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([[1,2,7],[3,4,8], [5, 6, 9]])\n",
    "print(tensor)\n",
    "print(torch.randperm(tensor.size(0)))\n",
    "tensor = tensor[torch.randperm(tensor.size(0))] # tensor[:, torch.randperm(tensor.size(1))] \n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71097cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0])\n",
      "tensor([[5., 6., 9.],\n",
      "        [3., 4., 8.],\n",
      "        [1., 2., 7.]])\n"
     ]
    }
   ],
   "source": [
    "# 翻转\n",
    "# narray = tensor.numpy()\n",
    "# print(narray)\n",
    "# narray = narray[::-1, :]\n",
    "# print(narray)\n",
    "\n",
    "tensor = torch.Tensor([[1,2,7],[3,4,8], [5, 6, 9]])\n",
    "\n",
    "# print(tensor.flip(dims=(1,)))\n",
    "\n",
    "print(torch.arange(tensor.size(0)-1, -1, -1))\n",
    "tensor = tensor[torch.arange(tensor.size(0)-1, -1, -1), :]\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd26ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor([1., 3.])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([1,2])\n",
    "tensor1 = tensor.clone()\n",
    "# tensor1[1]=3\n",
    "print(tensor)\n",
    "print(tensor1)\n",
    "\n",
    "# print(tensor.detach())\n",
    "# tensor.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40c66fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 4])\n",
      "3 4\n",
      "torch.Size([4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.rand(2,3,4)\n",
    "tensor2 = torch.rand(2,3,4)\n",
    "tensor_cat = torch.cat([tensor1, tensor2],dim=0)\n",
    "print(tensor_cat.shape)\n",
    "tensor_stack = torch.stack([tensor1, tensor2], dim=0)\n",
    "# print(*tensor_stack.shape[-2:])\n",
    "print(tensor_stack.reshape(-1, *tensor_stack.shape[-2:]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "926d77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(32,28,28)\n",
    "index=torch.unsqueeze(tensor, dim=1)\n",
    "print(index.shape)\n",
    "print(torch.squeeze(index).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21895945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.Tensor([0,1,4,0,6,0])\n",
    "print(tensor[torch.nonzero(tensor)]) # tensor[:]\n",
    "print(torch.nonzero(tensor==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cce520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True]],\n",
      "\n",
      "         [[True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True]],\n",
      "\n",
      "         [[True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True],\n",
      "          [True, True, True, True, True]]]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "import numpy as np\n",
    "feat = torch.randn(1,3,5,5)\n",
    "k = torch.randn(1,3,1,1)\n",
    "k_1 = k.expand(1,3,5,5)\n",
    "# print(k_1)\n",
    "print(feat * k_1 == feat * k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a43a9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.,  4.],\n",
      "        [26., 10.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [15.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.Tensor([[1,2],[3,4]])\n",
    "tensor2 = torch.Tensor([[2,2],[5,1]])\n",
    "print(torch.mm(tensor1, tensor2))\n",
    "# print(tensor1.matmul(tensor2))\n",
    "# print(torch.bmm(tensor1.unsqueeze(0), tensor2.unsqueeze(0)))\n",
    "\n",
    "print(tensor1*tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8028ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力网络\n",
    "# Squeeze-and-Excitation Networks， which is a channel attention block.\n",
    "class SE_Block(nn.Module):\n",
    "    def __init__(self, ch_in, reduction=16): \n",
    "        super(SE_Block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ch_in, ch_in // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ch_in // reduction, ch_in, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)  # squeeze operation\n",
    "        y = self.fc(y).view(b, c, 1, 1)  # FC obtains channel attention weight\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "# CBAM: Convolutional Block Attention Module\n",
    "# channel and spatial attention\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channel, ratio=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        # channel attention\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) \n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, in_channel // ratio, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channel // ratio, in_channel, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # spatial attention\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # b, c, w, h = x.size()\n",
    "        # channel attention\n",
    "        avg_fc = self.fc(self.avg_pool(x))\n",
    "        max_fc = self.fc(self.max_pool(x))\n",
    "        c_out = self.sigmoid(avg_fc + max_fc)\n",
    "        x_c = x * c_out\n",
    "\n",
    "        # spatial attention\n",
    "        x_mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        x_max, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([x_mean, x_max], dim=1)\n",
    "        x_s = self.sigmoid(self.conv(x_cat))\n",
    "        out = x_c * x_s\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323a31fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBAM(\n",
      "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (max_pool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      "  (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      ")\n",
      "Number of total parameter: 0.0001M\n",
      "torch.Size([1, 16, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "feat = torch.randn(1,16,28,28)\n",
    "\n",
    "# se = SE_Block(16)\n",
    "# print(se)\n",
    "# print(sum(p.numel() for p in se.parameters()))\n",
    "# print(\"Number of total parameter: %.4fM\" % (sum(p.numel() for p in se.parameters()) / 1e6))\n",
    "# feat_att = se(feat)\n",
    "# print(feat_att.shape)\n",
    "import math\n",
    "cbam = CBAM(16)\n",
    "print(cbam)\n",
    "print(\"Number of total parameter: %.4fM\" % (sum(p.numel() for p in cbam.parameters()) / 1e6))\n",
    "feat_att = cbam(feat)\n",
    "print(feat_att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771e92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional neural network (2 convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.se_att = CBAM(16)\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.se_att1 = SE_Block(32)\n",
    "        \n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "        # Init weights \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.se_att(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.se_att1(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d278605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7054762840270996\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "cnn = ConvNet()\n",
    "# print(cnn)\n",
    "img = torch.randn(1,1,28,28)\n",
    "feat = cnn(img)\n",
    "toc = time.time()-tic\n",
    "print(toc)\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd827151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common practise for initialization.\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out',\n",
    "                                      nonlinearity='relu')\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "    elif isinstance(layer, torch.nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(layer.weight, val=1.0)\n",
    "        torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "    elif isinstance(layer, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "\n",
    "# Initialization with given tensor.\n",
    "layer.weight = torch.nn.Parameter(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ef475",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model.load_state_dict(torch.load('*.pth.tar')) # map_location='cpu', strict=False\n",
    "\n",
    "# model_new代表新的模型\n",
    "# model_saved代表其他模型，比如用torch.load导入的已保存的模型\n",
    "model_new_dict = model_new.state_dict()\n",
    "model_common_dict = {k:v for k, v in model_saved.items() if k in model_new_dict.keys()}\n",
    "model_new_dict.update(model_common_dict)\n",
    "model_new.load_state_dict(model_new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练之前的数据处理 \n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(size=224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225)),\n",
    " ])\n",
    " val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9436aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 432)\n",
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('fig.jpg')\n",
    "print(img.size)\n",
    "img1 = torchvision.transforms.RandomResizedCrop(size=224)(img)\n",
    "img2 = torchvision.transforms.RandomHorizontalFlip()(img)\n",
    "img3 = torchvision.transforms.Resize(256)(img)\n",
    "img4 = torchvision.transforms.CenterCrop(224)(img)\n",
    "print(img1.size)\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(img)\n",
    "# # plt.title(\"原图\")\n",
    "# plt.subplot(2,3,2)\n",
    "# plt.imshow(img1)\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(img2)\n",
    "# plt.subplot(2,3,4)\n",
    "# plt.imshow(img3)\n",
    "# plt.subplot(2,3,5)\n",
    "# plt.imshow(img4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i ,(images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Loss: {}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7de1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the model\n",
    "model.eval()  # eval mode(batch norm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test accuracy of the model on the 10000 test images: {} %'\n",
    "          .format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15304596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(torch.nn.Moudle):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss = torch.mean((x - y) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度裁剪（gradient clipping）\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce learning rate when validation accuarcy plateau.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)\n",
    "for t in range(0, 80):\n",
    "    train(...)\n",
    "    val(...)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "# Cosine annealing learning rate.\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)\n",
    "# Reduce learning rate by 10 at given epochs.\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)\n",
    "for t in range(0, 80):\n",
    "    scheduler.step()    \n",
    "    train(...)\n",
    "    val(...)\n",
    "\n",
    "# Learning rate warmup by 10 epochs.\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)\n",
    "for t in range(0, 10):\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    val(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "df72e331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1\n",
      "1 0.09000000000000001\n",
      "2 0.08100000000000002\n",
      "3 0.007290000000000002\n"
     ]
    }
   ],
   "source": [
    "# 从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]\n",
    "optimizer = SGD(model, 0.1)\n",
    "scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "for epoch in range(4):\n",
    "    print(epoch, scheduler2.get_last_lr()[0])\n",
    "    optimizer.step()\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4344232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练可视化\n",
    "# pip install tensorboard\n",
    "# tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d926a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "04f7ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# ResNet GAP feature.\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# print(model)\n",
    "model = torch.nn.Sequential(collections.OrderedDict(\n",
    "    list(model.named_children())[:-1]))\n",
    "# print(model)\n",
    "\n",
    "img = Image.open('fig.jpg')\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(size=224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225)),\n",
    " ])\n",
    "img = transform(img)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    feat = model(img.unsqueeze(0))\n",
    "    print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3917d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微调全连接层\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(512, 100)  # Replace the last fc layer\n",
    "\n",
    "# 以较大学习率微调全连接层，较小学习率微调卷积层\n",
    "finetuned_parameters = list(map(id, model.fc.parameters()))\n",
    "conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)\n",
    "parameters = [{'params': conv_parameters, 'lr': 1e-3}, \n",
    "              {'params': model.fc.parameters()}]\n",
    "optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5c413",
   "metadata": {},
   "source": [
    "Minist for a example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aba1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)  # 使用随机化种子使神经网络的初始化每次都相同\n",
    "\n",
    "# 超参数\n",
    "EPOCH = 1  # 训练整批数据的次数\n",
    "BATCH_SIZE = 20 # 10 20 30, 8 16 32 64 \n",
    "LR = 0.001  # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载mnist手写数据集\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./data/',  # 保存或提取的位置  会放在当前文件夹中\n",
    "    train=True,  # true说明是用于训练的数据，false说明是用于测试的数据\n",
    "    transform=torchvision.transforms.ToTensor(),  # 转换PIL.Image or numpy.ndarray\n",
    "    download=True,\n",
    ")\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=False  # 表明是测试集\n",
    ")\n",
    "print(test_data)\n",
    "print(test_data.train_data[:2].shape)\n",
    "print(torch.unsqueeze(test_data.train_data, dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bcef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "# torch.unsqueeze(a) 是用来对数据维度进行扩充，这样shape就从(2000,28,28)->(2000,1,28,28) \n",
    "# 图像的pixel本来是0到255之间，除以255对图像进行归一化使取值范围在(0,1)\n",
    "test_x = torch.unsqueeze(test_data.train_data, dim=1).type(torch.FloatTensor)[:20] / 255 \n",
    "test_y = test_data.test_labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93496583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批训练 20个samples，1 channel，28x28 (20,1,28,28)\n",
    "# Torch中的DataLoader是用来包装数据的工具，它能帮我们有效迭代数据，这样就可以进行批训练\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True  #  训练时，为True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903464b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                  padding=padding, dilation=dilation, bias=bias), # padding = (kernel_size-1)/2\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "    \n",
    "\n",
    "# 简单的CNN (两层卷积层+全连接层)继承nn.Module这个模块\n",
    "class CNN(nn.Module):  \n",
    "    def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "        super().__init__()\n",
    "        # Conv2d->BN->ReLU\n",
    "        self.conv1 = conv_bn_relu(1, 16)   # 5, 1, 2 \n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = conv_bn_relu(16, 32)\n",
    "        # 建立全卷积连接层\n",
    "        self.fc = nn.Linear(32 * 7 * 7, 10)  # 输出是10个类\n",
    "\n",
    "        # Init weights \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.pooling(self.conv1(x))  \n",
    "        x = self.pooling(self.conv2(x))\n",
    "\n",
    "        # 把每一个批次的每一个输入都拉成一个维度，即(batch_size,32*7*7)\n",
    "        # 因为pytorch里特征的形式是[bs,channel,h,w]，所以x.size(0)就是batchsize\n",
    "        x = x.reshape(x.size(0), -1) #reshape view\n",
    "        output = self.fc(x) \n",
    "        return output # 32 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f083eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_paras_count(net):\n",
    "    \"\"\"cnn参数量统计\"\"\"\n",
    "    # Find total parameters and trainable parameters\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    print(\"Number of total parameter: %.2fM\" % (total_params / 1e6))\n",
    "    total_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    print(\"Number of trainable parameter: %.2fM\" % (total_trainable_params / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a99869",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_paras_count(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5883cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器选择Adam\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) #SGD adam adamW \n",
    "# 损失函数 \n",
    "loss_func = nn.CrossEntropyLoss()  # 目标标签是one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1981397",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = cnn(test_x)\n",
    "# print(test_output.shape)\n",
    "# print(torch.max(test_output, 1))\n",
    "# print(torch.max(test_output, 1)[1])\n",
    "pred_y = torch.max(test_output, 1)[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traing\n",
    "# 把x和y 都放入Variable中，然后放入cnn中计算output，最后再计算误差\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):  # 分配batch data\n",
    "        output = cnn(b_x)  # 先将数据放到cnn中计算output\n",
    "        loss = loss_func(output, b_y)  # 输出和真实标签的loss，二者位置不可颠倒\n",
    "        optimizer.zero_grad()  # 清除之前学到的梯度的参数\n",
    "        loss.backward()  # 反向传播，计算梯度\n",
    "        optimizer.step()  # 应用梯度\n",
    "            \n",
    "        if step % 100 == 0:\n",
    "            test_output = cnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == test_y.data.numpy()).sum() / test_y.size(0))\n",
    "            print('Epoch: ', epoch+1, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n",
    "\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')#保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ad47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型，调用时需将前面训练及保存模型的代码注释掉，否则会再训练一遍\n",
    "cnn.load_state_dict(torch.load('cnn.pkl'))\n",
    "cnn.eval()\n",
    "\n",
    "# print 10 predictions from test data\n",
    "inputs = test_x[:20]  # 测试32个数据\n",
    "test_output = cnn(inputs)\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "print('prediction number:',pred_y)  # 打印识别后的数字\n",
    "print('real number:', test_y[:20].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.shape)\n",
    "img = torchvision.utils.make_grid(inputs, padding=2)\n",
    "print(img.shape)\n",
    "img = img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "cv2.imshow('win', img)  # opencv显示需要识别的数据图片\n",
    "key = cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytracking]",
   "language": "python",
   "name": "pytracking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}